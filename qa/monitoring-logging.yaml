config:
  questionDelay: 20
  answerDelay: 5
questions:
- question: What is Prometheus, and why is it used?
  answer: 'Prometheus is an open-source monitoring and alerting system used to collect metrics from applications and infrastructure.
    It is widely used because of its pull-based model, powerful query language (PromQL), and time-series database capabilities.

    Example Use Case:

    Monitoring CPU, memory, and network usage

    Collecting application performance metrics

    Alerting on high error rates or latency'
- question: How does Prometheus collect data?
  answer: 'Prometheus pulls metrics from target endpoints exposed via HTTP at /metrics. The targets can be defined in a static
    configuration or discovered dynamically (e.g., Kubernetes service discovery).

    Example scrape configuration (prometheus.yml):

    scrape_configs:

    - job_name: ''node_exporter''

    static_configs:

    - targets: [''localhost:9100'']'
- question: What is PromQL?
  answer: 'PromQL (Prometheus Query Language) is used to query and analyze metrics stored in Prometheus. It enables users
    to create alerts, dashboards, and graphs.

    Example Queries:

    CPU usage:

    promql

    node_cpu_seconds_total{mode="user"} / sum(node_cpu_seconds_total) * 100

    Request rate:

    promql

    rate(http_requests_total[5m])'
- question: What are Prometheus exporters?
  answer: 'Exporters are agents that collect and expose metrics from various applications and systems.

    Common Exporters:

    Node Exporter (system metrics)

    Blackbox Exporter (network probes)

    MySQL Exporter (database metrics)'
- question: How do you set up an alert in Prometheus?
  answer: 'Alerts are configured in alerting_rules.yml and evaluated by the Alertmanager.

    Example Rule:

    groups:

    - name: instance_down

    rules:

    - alert: InstanceDown

    expr: up == 0

    for: 5m

    labels:

    severity: critical

    annotations:

    description: "Instance {{ $labels.instance }} is down."'
- question: What is Grafana?
  answer: Grafana is an open-source analytics and visualization tool used to create interactive dashboards for monitoring
    data from Prometheus, ELK, and other sources.
- question: How do you connect Grafana to Prometheus?
  answer: 'Login to Grafana (http://localhost:3000).

    Navigate to "Configuration" → "Data Sources".

    Select Prometheus as the data source.

    Enter Prometheus URL (http://localhost:9090).

    Click Save & Test.'
- question: What are Grafana Panels?
  answer: 'Panels are visual components in Grafana used to display data in various formats:

    Graph Panel: Time-series data visualization

    Single Stat Panel: Displays a single numeric value

    Table Panel: Tabular data display'
- question: How do you create alerts in Grafana?
  answer: 'Select a panel.

    Click "Edit" → "Alert".

    Define a condition using PromQL queries.

    Set the evaluation interval (e.g., every 1m).

    Configure the alert notification (Slack, Email, etc.).'
- question: How do you configure a Grafana dashboard using JSON?
  answer: 'Export and import dashboards using JSON files.

    Example JSON snippet:

    {

    "panels": [

    {

    "type": "graph",

    "title": "CPU Usage",

    "targets": [

    { "expr": "node_cpu_seconds_total", "format": "time_series" }

    ]

    }

    ]

    }'
- question: What is the ELK Stack?
  answer: 'The ELK Stack consists of:

    Elasticsearch (search and analytics engine)

    Logstash (log processing pipeline)

    Kibana (visualization tool)'
- question: What is the role of Elasticsearch in ELK?
  answer: Elasticsearch is a NoSQL, distributed search engine used to store, search, and analyze log data.
- question: How does Logstash work?
  answer: 'Logstash processes logs using a pipeline:

    Input: Reads logs (from files, databases, Kafka, etc.)

    Filter: Transforms logs (parse JSON, remove sensitive data)

    Output: Sends logs to Elasticsearch or other storage

    Example Logstash Configuration:

    input { file { path => "/var/log/syslog" } }

    filter { grok { match => { "message" => "%{SYSLOGTIMESTAMP:timestamp}" } } }

    output { elasticsearch { hosts => ["localhost:9200"] } }'
- question: What is Kibana used for?
  answer: 'Kibana is used to visualize and explore log data stored in Elasticsearch. It provides features like:

    Dashboards: Custom data visualizations

    Discover: Search raw logs

    Alerts: Set up log-based alerts'
- question: How do you install the ELK stack?
  answer: 'Install Elasticsearch, Logstash, and Kibana:

    # Install Elasticsearch

    sudo apt install elasticsearch

    # Install Logstash

    sudo apt install logstash

    # Install Kibana

    sudo apt install kibana

    Start services:

    sudo systemctl start elasticsearch logstash kibana'
- question: What is an Index in Elasticsearch?
  answer: 'An index in Elasticsearch is like a database table that stores documents.

    Example:

    curl -X PUT "localhost:9200/logs"'
- question: How do you send logs from Logstash to Elasticsearch?
  answer: 'Define an output plugin in Logstash configuration:

    output {

    elasticsearch {

    hosts => ["http://localhost:9200"]

    index => "logs-%{+YYYY.MM.dd}"

    }

    }'
- question: What is a Kibana Visualization?
  answer: 'A Kibana Visualization is a graph, chart, or table displaying log data.

    Example Visualizations:

    Bar Chart (Logs per hour)

    Pie Chart (Error types distribution)

    Line Chart (CPU usage over time)'
- question: What is Filebeat?
  answer: 'Filebeat is a lightweight log shipper that forwards logs to Logstash or Elasticsearch.

    Example Filebeat Configuration:

    filebeat.inputs:

    - type: log

    paths:

    - "/var/log/syslog"

    output.elasticsearch:

    hosts: ["localhost:9200"]'
- question: What is the difference between Logstash and Filebeat?
  answer: 'Logstash: Heavyweight, processes logs with complex transformations

    Filebeat: Lightweight, only forwards logs with minimal processing'
- question: What is the difference between Pull and Push monitoring models?
  answer: 'Pull Model (Prometheus) → The monitoring system requests data from targets at regular intervals.

    Push Model (StatsD, InfluxDB) → The target system sends data to a central monitoring system.

    Prometheus uses a pull model because it provides better control over scraping intervals, avoids data duplication, and
    reduces unnecessary load on monitored systems. However, in some cases (e.g., short-lived jobs), Prometheus Pushgateway
    can be used to support push-based metrics.'
- question: How does Prometheus handle high-cardinality data?
  answer: 'Prometheus stores time-series data efficiently, but high-cardinality metrics (many unique label combinations) can
    cause excessive memory and storage usage. Best practices include:

    Avoid unnecessary labels (e.g., user_id or request_id).

    Use histograms and summaries instead of tracking individual events.

    Enable retention policies and downsampling for old data.'
- question: What are Recording Rules in Prometheus?
  answer: 'Recording Rules allow precomputing and storing frequently used queries as new time-series metrics. This improves
    query performance.

    Example:

    groups:

    - name: response_time_rules

    rules:

    - record: instance:response_time:avg

    expr: avg(rate(http_request_duration_seconds[5m]))

    This stores the average request duration as instance:response_time:avg, making future queries faster.'
- question: What is Thanos, and how does it complement Prometheus?
  answer: 'Thanos extends Prometheus for scalability, long-term storage, and high availability. It:

    Provides deduplication across multiple Prometheus instances.

    Enables object storage support (e.g., S3, GCS).

    Allows querying across multiple Prometheus servers via a single query layer.

    Thanos is useful in multi-cluster environments where Prometheus instances are spread across multiple regions or clouds.'
- question: How do you handle Prometheus high availability (HA)?
  answer: 'Prometheus is a single-node system by design, but HA can be achieved by:

    Running multiple Prometheus replicas (scraping the same targets).

    Using Thanos or Cortex for deduplication and query federation.

    Storing time-series data externally (e.g., in S3, Bigtable).'
- question: How do you enable authentication in Grafana?
  answer: 'Grafana supports multiple authentication methods:

    Basic authentication (default).

    OAuth providers (Google, GitHub, Azure AD, etc.).

    LDAP authentication for enterprise use.

    To enable OAuth authentication, modify grafana.ini:

    [auth.github]

    enabled = true

    client_id = YOUR_CLIENT_ID

    client_secret = YOUR_CLIENT_SECRET'
- question: What are Templating Variables in Grafana?
  answer: 'Templating allows users to create dynamic dashboards by using variables. Instead of hardcoding values, users can
    select values from dropdown menus.

    Example:

    rate(http_requests_total{job="$service"}[5m])

    Here, $service is a variable that can be selected from a dropdown list in Grafana.'
- question: How do you set up Grafana provisioning?
  answer: 'Grafana supports automated provisioning of dashboards and data sources using YAML configuration files.

    Example datasource.yaml:

    apiVersion: 1

    datasources:

    - name: Prometheus

    type: prometheus

    url: http://prometheus:9090

    access: proxy'
- question: What are Grafana Loki and Promtail?
  answer: 'Loki is Grafana''s log aggregation system, similar to Elasticsearch but optimized for Kubernetes and microservices.

    Promtail is the log collection agent for pushing logs to Loki.

    Promtail collects logs from /var/log and forwards them to Loki.'
- question: How can you monitor Kubernetes with Grafana?
  answer: 'Use kube-prometheus-stack, which includes:

    Prometheus Operator (for Kubernetes metrics).

    Grafana dashboards for cluster monitoring.

    Node Exporter and Kube-State-Metrics for detailed node/pod-level metrics.'
- question: What is an Elasticsearch Shard, and why is it important?
  answer: 'An Elasticsearch shard is a subdivision of an index. Each index is split into shards to allow parallel processing
    and redundancy.

    Primary Shards: Store original data.

    Replica Shards: Duplicates of primary shards for fault tolerance.

    Example:

    curl -X PUT "localhost:9200/logs?pretty" -H ''Content-Type: application/json'' -d''

    {

    "settings": { "number_of_shards": 3, "number_of_replicas": 2 }

    }''

    This creates an index with 3 primary and 2 replica shards.'
- question: What is Index Lifecycle Management (ILM) in Elasticsearch?
  answer: 'ILM automates index retention policies, ensuring efficient storage use. Stages include:

    Hot Phase: Frequent reads/writes.

    Warm Phase: Less frequent queries.

    Cold Phase: Rarely accessed data.

    Delete Phase: Data deletion.

    ILM is useful for managing log retention in ELK stacks.'
- question: How do you configure Logstash pipelines?
  answer: 'Logstash uses a pipeline of input → filter → output.

    Example logstash.conf:

    input {

    beats {

    port => 5044

    }

    }

    filter {

    grok { match => { "message" => "%{TIMESTAMP_ISO8601:timestamp}" } }

    }

    output {

    elasticsearch { hosts => ["localhost:9200"] }

    }

    This pipeline processes logs from Filebeat → Logstash → Elasticsearch.'
- question: What are Kibana Canvas and Lens?
  answer: 'Canvas → Used for creating custom, highly stylized reports and presentations.

    Lens → Drag-and-drop interface for creating advanced visualizations easily.'
- question: How do you configure Kibana security?
  answer: 'Enable authentication in kibana.yml:

    xpack.security.enabled: true

    elasticsearch.username: "kibana"

    elasticsearch.password: "changeme"

    Use role-based access control (RBAC) to restrict access.'
- question: What is Beats in the ELK stack?
  answer: 'Beats are lightweight data shippers for sending logs, metrics, and security data to ELK.

    Filebeat: Log shipping.

    Metricbeat: System metrics.

    Packetbeat: Network monitoring.'
- question: What is Curator in Elasticsearch?
  answer: Curator is a tool for managing Elasticsearch indices, used for deleting old indices, snapshot backups, and optimizing
    performance.
- question: How do you integrate Prometheus and ELK Stack?
  answer: Use Metricbeat to collect system metrics and send them to Elasticsearch, while Prometheus Node Exporter collects
    Prometheus-compatible metrics.
- question: What is a Slow Query in Elasticsearch?
  answer: 'A slow query is a query that takes too long to execute, often due to large data scans or missing indexes. Enable
    slow query logs to debug:

    PUT _settings

    {

    "index.search.slowlog.threshold.query.warn": "2s"

    }'
- question: What is the ELK alternative to Prometheus and Grafana?
  answer: 'Prometheus + Grafana → Metrics-based monitoring.

    ELK Stack (Elasticsearch, Logstash, Kibana) → Log-based monitoring.

    Alternative: OpenTelemetry, Loki, and InfluxDB.'
- question: How do you scale Prometheus for a large environment?
  answer: 'Prometheus is a single-node system, so for large environments:

    Use multiple Prometheus instances scraping different targets.

    Federation: Create a parent Prometheus that scrapes aggregated metrics from child Prometheus instances.

    Remote storage: Use Thanos, Cortex, or Mimir to store metrics in scalable object storage (S3, GCS).

    Sharding: Distribute scraping targets across Prometheus instances using load balancing tools like Kube StatefulSets.'
- question: How does Prometheus handle stale or missing metrics?
  answer: 'Stale markers: Prometheus marks time-series data as stale if a target stops reporting metrics.

    Absent function (absent()): Used in PromQL to detect missing metrics.

    Dead Man’s Switch: A constant alert (e.g., ALWAYS_ON) ensures the alerting system is functional.

    Example:

    absent(up{job="my_service"})

    Triggers an alert if up{job="my_service"} is missing.'
- question: What is Prometheus WAL (Write-Ahead Log) and its purpose?
  answer: 'The Write-Ahead Log (WAL) in Prometheus:

    Stores data on disk before committing it to TSDB (Time-Series Database).

    Reduces data loss during crashes.

    WAL files are stored in /data/wal/ and help recover metrics quickly after a restart.'
- question: What are Histogram and Summary metrics in Prometheus?
  answer: 'Both are used for measuring latency and response time:

    Histogram: Buckets data into predefined ranges, allowing percentiles to be calculated later.

    Summary: Precomputes percentiles but cannot be aggregated across instances.

    Example (Histogram metric):

    histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

    This calculates the 95th percentile response time.'
- question: How do you secure Prometheus endpoints?
  answer: 'Enable authentication & TLS via a reverse proxy (Nginx, Traefik).

    Use RBAC (Role-Based Access Control) in Kubernetes for limiting access.

    Set up network policies to restrict Prometheus access.

    Example: Using basic auth with Nginx:

    server {

    listen 9090;

    location / {

    auth_basic "Restricted";

    auth_basic_user_file /etc/nginx/.htpasswd;

    }

    }'
- question: How do you monitor Prometheus itself using Grafana?
  answer: 'Enable the built-in Prometheus self-metrics endpoint (/metrics).

    Use dashboards to monitor scrape latency, TSDB memory usage, query duration.

    Use the Prometheus Federation API to get meta-metrics.'
- question: What are Grafana Annotations and how are they useful?
  answer: 'Annotations mark events (deployments, incidents, downtimes) on Grafana graphs for better visualization.

    Example: Mark a Kubernetes deployment event in Grafana.'
- question: How do you configure Grafana for multi-tenancy?
  answer: 'Organizations: Create multiple teams with separate dashboards.

    Data source permissions: Restrict access at the data-source level.

    Multi-instance deployment: Run separate Grafana instances for different teams.'
- question: What is Alerting in Grafana and how does it work?
  answer: 'Grafana alerts monitor query conditions.

    Alert states: OK, Pending, Alerting, No Data.

    Notification channels: Slack, PagerDuty, Email, Webhooks.

    Example Grafana alert condition:

    avg(http_requests_total) > 1000 → Sends an alert if requests exceed 1000.'
- question: How does Loki compare with Elasticsearch for logging?
  answer: 'Feature

    Loki

    Elasticsearch

    Storage

    Compressed logs

    Full-text index

    Querying

    Label-based

    Query DSL

    Performance

    Faster (optimized for Kubernetes)

    Heavy resource usage

    Loki is recommended for lightweight, Kubernetes-native logging, while Elasticsearch is better for complex log analysis.'
- question: What is the Hot-Warm-Cold architecture in Elasticsearch?
  answer: 'This strategy optimizes storage cost:

    Hot Nodes → Store recent, frequently queried data.

    Warm Nodes → Store older logs with infrequent access.

    Cold Nodes → Store archived logs for long-term retention.'
- question: How do you reduce indexing pressure in Elasticsearch?
  answer: 'Use ILM (Index Lifecycle Management).

    Optimize shard count (Avoid too many small shards).

    Increase refresh intervals (index.refresh_interval: 30s).'
- question: How does Logstash manage backpressure?
  answer: 'Persistent Queues → Buffer data before sending to Elasticsearch.

    Dead Letter Queue (DLQ) → Stores failed events for reprocessing.

    Example:

    queue.type: persisted

    queue.max_bytes: 1gb'
- question: What are Query Caching strategies in Elasticsearch?
  answer: 'Request cache: Stores query results.

    Shard request cache: Caches aggregations and filters.

    Doc value cache: Optimizes sorting and aggregations.'
- question: How do you use Kibana for anomaly detection?
  answer: 'Machine Learning Jobs → Identify unusual trends in logs.

    SIEM (Security Information and Event Management) → Detect security threats.

    Example anomaly detection job:

    {

    "analysis_config": {

    "bucket_span": "15m",

    "detectors": [{ "function": "mean", "field_name": "cpu_usage" }]

    }

    }'
- question: How do you secure Elasticsearch clusters?
  answer: 'Enable TLS (xpack.security.enabled: true).

    Use API Key authentication.

    Implement firewall rules to restrict access.'
- question: How do you integrate Prometheus with Elasticsearch?
  answer: 'Use Metricbeat to push Prometheus data into Elasticsearch.

    Use Grafana to visualize both Prometheus & ELK logs.

    Example Metricbeat configuration:

    metricbeat.modules:

    - module: prometheus

    metricsets: ["collector"]

    host: "localhost:9090"'
- question: How do you optimize Elasticsearch queries for performance?
  answer: 'Use filters (term, match_phrase) instead of full-text search.

    Avoid wildcard (*) searches.

    Use doc_values for sorting and aggregations.'
- question: How do you implement centralized logging in Kubernetes?
  answer: 'Use Fluentd/Filebeat to collect logs.

    Send logs to Elasticsearch or Loki.

    Monitor logs via Kibana or Grafana dashboards.

    Example Fluentd configuration:

    @type elasticsearch

    host elasticsearch

    logstash_format true'
- question: What are the best practices for log retention and compliance?
  answer: 'Use ILM to delete old logs automatically.

    Encrypt sensitive logs (xpack.security).

    Mask PII data before indexing logs.

    Set audit logs for security compliance.'
